## PCA(Principal Component Analysis)

	- 주된 성분을 찾아보는 것이 PCA의 목표

### Dimensionality Reduction

- 사진을 찍었을 때, 가장 높은 성분을 가지는 차원을 찾아야 한다
- Dimension이 너무 올라가면 표현과 계산이 어렵다
- 주 성분으로 표현을 하면, 차원을 확 줄일 수 있다
  - 최대한 겹침이 없는 쪽으로 찾아야한다

#### Variance

- 변수들간의 평균 - 변수의 값의 제곱

- co-variance(공분산) : 다른 변수와의 관계도 본다 :arrow_forward: 기울기가 달라진다
  - matrix 

    var(X) 	Cov(X,Y)

    Cov(X,Y)  var(Y)

#### Linear Transformation

- Eigenvector : 방향

- Eigenvalue : 커진 정도, 클수록 많이 분포 돼 있다는 뜻
  - 차원 개수 만큼의 Eigenvalue가 나온다

### Steps

1. From set of face data (training data)
   - N * 1 벡터로 변환
2. Subtract mean
3. Form the matrix And compute C(공분산 행렬)
4. Compute the eigenvalues of C 
5. Compute the eigenvectors of C
6. (Dimensionality reduction step) Keep only the terms corresponding the K largest eigenvalues

### Auto Encoder

- input - (encoding) > network(중요한 특징을 추출) - (decoding) > output
  - loss function을 이용해 학습