## 다층 퍼셉트론

- 퍼셉트론은 선형 분류기라는 **한계**

- 선형 분리 불가능한 상황에서는 일정한 양의 오류

  ![1570362881401](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570362881401.png)

  - XOR 문제에서는 75%가 정확률 한계

- 1969년 민스키의 Perceptrons : 극복 방안 제시, 당시 기술로 실현 불가능

- 1974년 웨어보스의 박사 논문에서 오류 역전파 알고리즘 제안

- 1986년 루멜하트의 저서 Parallel Distributed Processing 다층 퍼셉트론 이론 정립, 신경망 부활

- **핵심 아이디어**

  - **은닉층**을 둔다. 은닉층은 원래 특징 공간을 분류하는 데 훨씬 **유리한 새로운 특징 공간**으로 변환
  - **시그모이드 활성함수를 도입**한다
    - 계단함수를 활성함수로 사용하면 경성 의사결정에 해당
    - 다층 퍼셉트론은 연성 의사결정이 가능한 **시그모이드 활성함수** 사용
      - **출력이 연속값**이고, 출력을 신뢰도로 간주함으로써 더 융통성 있게 의사결정
  - 오류 역전파 알고리즘 사용
    - 역방향으로 진행하면서 한 번에 한 층 씩 **그레디언트를 계산하고 가중치를 갱신**

### 특징 공간 변환

- 퍼셉트론 2개를 사용한 XOR 문제의 해결

  - 퍼셉트론 1과 퍼셉트론 2가 모두 +1이면 O 부류, 아니면 ㅁ 부류

    ![1570363383880](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570363383880.png)

- 퍼셉트론 2개를 **병렬 결합**하면

  - **원래 공간** ![1570363407410](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570363407410.png)를 **새로운 특징 공간** ![1570363434635](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570363434635.png)로 **변환**

  - 새로운 특징 공간 z에서는 선형 분리 가능함

    ![1570363510518](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570363510518.png)

  - 사람이 수작업으로 특징 학습을 수행한 것과 유사함

- 이후, 퍼셉트론 1개를 순차 결합하면

  - 새로운 특징 공간 z에서 선형 분리를 수행하는 퍼셉트론3을 순차 결합하면 다층 퍼셉트론

    ![1570363638719](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570363638719.png)

  - 이 다층 퍼셉트론은 훈련집합에 있는 4개 샘플 (0,0) (0,1) (1,0) (1,1)을 제대로 분류하는가?

- 다층 퍼셉트론의 용량

  - 3개 퍼셉트론을 결합하면, 2차원 공간을 7개 영역으로 나누고 각 영역을 3차원 점으로 변환

  - 활섬함수 ![1570363703120](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570363703120.png)로 계단함수를 사용하므로 영역을 점으로 변환

    ![1570363744315](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570363744315.png)

  - 일반화하여, **p개 퍼셉트론을 결합**하면 **p차원 공간으로 변환**

    - ![1570363765352](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570363765352.png)**개의 영역**으로 **분할**

  ![1570363831307](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570363831307.png)

### 활성 함수

- **딱딱한 공간 분할**과 **부드러운 공간 분할**

  - 계단함수는 딱딱한 의사결정(영역을 점으로 변환)
  - 나머지 활성함수는 부드러운 의사결정(영역을 영역으로 변환)

  ![1570363955923](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570363955923.png)

- 대표적인 **비선형 함수**인 **시그모이드**를 활성함수로 사용

  ![1570364008835](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570364008835.png)

- **활성 함수**에 따른 다층 퍼셉트론의 **공간 분할 능력 변화**

  ![1570364041720](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570364041720.png)

- 신경망이 사용하는 **다양한 활성함수**

  - 로지스틱 시그모이드와 하이퍼볼릭 탄젠트는 a가 커질수록 계단함수에 가까워짐

  - 모두 1차 도함수 계산이 빠름 (특히 ReLU는 비교 연산 한 번)

    ![1570364098389](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570364098389.png)

  - 퍼셉트론은 계단함수, 다층 퍼셉트론은 로지스틱 시그모이드와 하이퍼볼릭 탄젠트, **딥러닝**은 **ReLU**(Rectified Linear Activation)를 주로 사용

- 일반적으로 은닉층에서 **로지스틱 시그모이드**를 활성함수로 많이 사용

  - **시그모이드의 넓은 포화곡선**은 **그레디언트 기반한 학습을 어렵게 함**

  - ReLU 대두됨

    ![1570364218875](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570364218875.png)

### 구조

- 입력층-은닉층-출력층의 2층 구조

  - d+1개의 입력 노드(d는 특징의 개수). c개의 출력 노드(c는 부류 개수)

  - p개의 은닉 노드 : p는 하이퍼 매개변수(사용자가 정해주는 매개변수)

    - p가 **너무 크면 과잉적합**, **너무 작으면 과소적합** :arrow_forward: **하이퍼 매개변수 최적화**

    ![1570364324969](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570364324969.png)

  - b는 3층구조

- 다층 퍼셉트론의 **매개변수** (**가중치**)

  - **입력층 - 은닉층**을 연결하는 ![1570364409874](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570364409874.png)

  - **은닉층 - 출력층**을 연결하는 ![1570364428004](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570364428004.png)

    ![1570364441584](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570364441584.png)

  - 일반화하면 ![1570364462354](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570364462354.png)와 연결하는 가중치

    - 입력층을 0번째 은닉층, 출력층을 마지막 은닉층으로 간주

### 동작

- **특징 벡터** x를 **출력 벡터** o로 **매핑하는 함수**로 간주할 수 있음

  ![1570364575464](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570364575464.png)

- **깊은** **신경망**은 ![1570364592673](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570364592673.png)(딥러닝)

- 노드가 수행하는 연산을 구체적으로 쓰면

  ![1570364622355](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570364622355.png)

- 다층 퍼셉트론의 동작을 행렬로 표기하면

  ![1570364705804](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570364705804.png)

- **은닉층**은 **특징 추출기**

  - 은닉층은 **특징 벡터**를 분류에 더 유리한 **새로운 특징 공간으로 변환**

  - 현대 기계 학습에서는 **특징 학습**이라 (feature learning 혹은 data-driven features) 부름

    - 딥러닝은 더 많은 단계를 거쳐 특징학습을 함

    ![1570364775054](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570364775054.png)

- 기본 구조

  ![1570364817056](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570364817056.png)

  - **범용적 근사 이론**
    - 하나의 **은닉**층은 **함수의 근사**를 표현
    - **다층 퍼셉트론**도 **공간을 변환하는 함수를 근사**함
  - **얕은 은닉층의 구조**보다
    - 지수적으로 더 넓은 폭이 필요할 수 있음
    - 더 과잉적합 되기 쉬움
    - 일반적으로 **깊은 은닉층의 구조가 좋은 성능**을 가짐

- **은닉층의 깊이**에 따른 **이점**

  - **지수의 표현**

    ![1570364956994](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570364956994.png)

    - 각 **은닉층**은 입력 **공간을 어디서 접을지 지정** :arrow_forward: 지수적으로 **많은 선형적인 영역 조각들**

  - **일반화 성능 향상**과 **과잉적합 해소**

    ![1570365033523](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365033523.png)

## 오류 역전파 알고리즘

### 목적함수의 정의

- 훈련집합

  - 특징 벡터 집합 ![1570365108888](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365108888.png)과 부류 벡터 집합 ![1570365121462](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365121462.png)

  - **부류 벡터**는 **원핫 코드**로 표현됨, 즉 ![1570365140730](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365140730.png)

  - 설계 행렬로 쓰면

    ![1570365154157](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365154157.png)

- 기계 학습의 목표

  - 모든 **샘플을 옳게 분류**하는 **함수 f를 찾는 일**

    ![1570365215974](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365215974.png)

- 목적함수

  - **평균 제곱 오차**로 정의

    ![1570365277679](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365277679.png)

- **연산 그래프**의 예 : 연산을 그래프로 표현

  ![1570365334765](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365334765.png)

### 오류 역전파 알고리즘 설계

- 간단한 오류 역전파의 연산 그래프 예

  ![1570365363593](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365363593.png)

- **오류 역전파 미분의 연쇄 법칙을 이용**

  - **연쇄 법칙**

    - 수인 경우 : ![1570365407778](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365407778.png)
    - **벡터인 경우** : ![1570365422446](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365422446.png)

    ![1570365438096](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365438096.png)

    ![1570365449115](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365449115.png)

- **야코비안 행렬과 그레디언트를 곱한 연쇄 법칙**을 얻어서 구해짐

  ![1570365487503](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365487503.png)

- 연쇄법칙의 **구현**

  - **반복되는 부분식들**을 저장하거나 **재연산을 최소화**

    - ex) 동적 프로그래밍
    - 연산속도와 저장 공간의 Trade-off

    ![1570365609344](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365609344.png)

  - 그레디언트 계산을 위한 연산 그래프 예

    ![1570365597272](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365597272.png)

- 식 (3.19)의 목적함수를 다시 쓰면

  - 2층 퍼셉트론의 경우 ![1570365648587](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365648587.png)

    ![1570365659876](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365659876.png)

- 경사하강법

  ![1570365674144](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365674144.png)

- 식  (3.21)을 알고리즘 형태로 쓰면

  ![1570365701241](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365701241.png)

- 오류 역전파의 **유도**

  - 알고리즘 3-3의 라인 6을 위한 도함수 값 ![1570365744075](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365744075.png)의 계산 과정
  - 먼저 ![1570365830468](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365830468.png)를 구성하는 ![1570365772005](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365772005.png)로 미분하면,

  ![1570365791586](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365791586.png)

  - .![1570365852263](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365852263.png)

    ![1570365896791](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365896791.png)

  - 지금까지 유도한 식을 정리하면

    ![1570365920912](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365920912.png)

  - 오류 역전파 알고리즘

    - 식 (3.22) ~ (3.25)를 이용하여 **출력층의 오류**를 **역방향(왼쪽)으로 전파**하며 **그레이디언트를 계산**하는 알고리즘

- 역전파 분해

  ![1570365972702](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365972702.png)

- **단일 노드**의 역전파 예

  ![1570365990918](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570365990918.png)

- **곱셈**의 역전파 예

  ![1570366006796](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366006796.png)

- 곱셈의 역전파 PyTorch 구현 예

  ![1570366028974](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366028974.png)

- 덧셈의 역전파 예

  ![1570366042693](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366042693.png)

- **시그모이드**의 역전파 예

  ![1570366057121](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366057121.png)

- **최대화**의 역전파 예

  ![1570366078428](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366078428.png)

- **전개**의 역전파 예

  ![1570366095597](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366095597.png)

  ![1570366110465](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366110465.png)

- 역전파 주요 예

  ![1570366126276](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366126276.png)

- **실제 역전파** 예

  ![1570366140291](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366140291.png)

- 역전파의 간단한 구현

  ![1570366161857](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366161857.png)

### 오류 역전파를 이용한 학습 알고리즘

- 식 (3.22) ~ (3.25)를 이용한 **스토캐스틱 경사 하강법**

![1570366295082](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366295082.png)

- 임의 샘플링 방식으로 바꾸려면

  ![1570366341939](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366341939.png)

- 도함수의 종류

  ![1570366352790](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366352790.png)

- 행렬 표기 : GPU를 사용한 **고속 행렬 연산**에 적합

  ![1570366376483](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366376483.png)

## 미니배치 스토캐스틱 경사 하강법

- **미니배치** 방식

  - 한번에 t개의 샘플을 처리함 (t는 미니배치 크기)
    - t = 1이면 스토캐스틱 경사 하강법
    - t = n이면 배치 경사 하강법
  - 미니배치 방식은 보통 t = 수십 ~ 수백
    - 그레이디언트의 잡음을 줄여주는 효과 때문에 **수렴이 빨라짐**
    - GPU를 사용한 **병렬처리에도 유리함**
  - 현대 기계 학습은 **미니배치를 표준**처럼 여겨 널리 사용함

  ![1570366505005](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366505005.png)

- 행렬의 역전파를 위한 도함수

  ![1570366527211](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366527211.png)

## 다층 퍼셉트론에 의한 인식

- 예측 단계 (또는 테스트 단계)

  - 학습을 마친 후 현장 설치하여 사용(또는 테스트 집합으로 성능 테스트)

    ![1570366599879](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366599879.png)

  - 라인 6을 수식으로 표현하면 ![1570366611475](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366611475.png)
  - 전방 계산 한번만 사용하므로 빠름

## 다층 퍼셉트론의 특성

### 오류 역전파 알고리즘의 빠른 속도

- 연산 횟수 비교

  ![1570366691847](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366691847.png)

  - 오류 역전파는 전방 계산보다 약 1.5~2배의 시간 소요 :arrow_forward: 빠른 계산 가능
  - 하지만 학습 알고리즘은 수렴할 때 까지 **오류 역전파를 반복**해야 하므로 점근적 시간복잡도는
    ![1570366761802](C:\Users\user\AppData\Roaming\Typora\typora-user-images\1570366761802.png)
  - 에포크는 전체 학습 집합을 수행한 단위

### 모든 함수를 정확하게 근사할 수 있는 능력

- 호닉의 주장[Hornik1989]
  - 은닉층을 하나만 가진 다층 퍼셉트론은 범용근사자
    - **은닉 노드가 충분히 많다면** 활성함수로 무엇을 사용하든 표준 다층 퍼셉트론은 어떤 함수라도 원하는 정확도만큼 **근사화**할 수 있다
  - 은닉 노드를 무수히 많게 할 수 없으므로, 실질적으로는 복잡한 구조의 데이터에서는 성능 한계

### 성능 향상을 위한 휴리스틱의 중요성

- 순수한 최적화 알고리즘으로 높은 성능 불가능
  - 데이터 희소성, 잡음, 미숙한 신경망 구조 등의 이유
  - **성능 향상**을 위한 갖가지 **휴리스틱**을 개발하고 공유함
- 휴리스틱 개발에서 중요 쟁점
  - 아키텍쳐
    - 은닉층과 은닉노드의 개수를 정해야 한다. 은닉층과 은닉 노드를 놀리면 신경망의 용량은 커지는 대신, 추정할 매개변수가 많아지고 학습 과정에서 과잉적합할 가능성이 커진다.
    - 현대 기계학습은 복잡한 모델을 사용하되, 적절한 규제기법을 적용하는 경향이 있음
  - 초깃값
    - 가중치를 초기화할때 보통 난수를 생성하여 설정하는데, 값의 범위와 분포가 중요
  - 학습률
    - 처음부터 끝까지 같은 학습률을 사용하는 방식과 처음에는 큰 값으로 시작하고 점점 줄이는 적용적 방식이 있음
  - 활성함수
    - 초창기 다층 퍼셉트론은 주로 로지스틱 시그모이드나 tanh 함수를 사용했는데, 은닉층의 개수를 늘림에 따라 그레이디언트 소멸과 같은 몇가지 문제가 발생한다
    - 깊은 신경망은 주로 ReLU 함수를 사용
- 실용적인 성능
  - 1980 ~ 1990년대에 다층 퍼셉트론은 실용 시스템 제작에 크게 기여
    - 인쇄/필기 문자 인식으로 우편물 자동 분류기, 전표 인식기, 자동차 번호판 인식기 등
    - 음성 인식, 게임, 주가 예측, 정보 검색, 의료 진단, 유전자 검색, 반도체 결함 검사 등
- 하지만 한계 노출
  - 잡음이 섞인 상황에서 음성인식 성능 저하
  - 필기 주소 인식 능력 저하
  - 바둑 등의 복잡한 문제에서의 한계
- 이러한 한계를 **딥러닝**은 극복함