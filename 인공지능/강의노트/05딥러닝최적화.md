# 딥러닝 최적화

- 과학과 공학에서 최적화
- 기계학습의 최적화도 매우 복잡함
  - 훈련집합으로 학습을 마친 후, 현장에서 발생하는 새로운 샘플을 잘 예측해야 함
    - 즉 **일반화 능력**이 좋아야 함
    - **훈련집합**은 전체 데이터(실제로 알 수 없음)의 대리자 역할
    - **검증집합**은 테스트집합의 대리자 역할
    - 오차제곱의 평균, 로그우도와 같은 **목적함수**도 궁극 목표인 정확률(판단 기준)의 대리자 역할
- 기계학습의 최적화가 어려운 이유
  - 대리자 관계
  - 목적함수의 비볼록 성질, 고차원 특징 공간, 데이터의 희소성 등
    - 볼록 성질을 가지는 목적함수도 있긴하다(MSE) - 최소값이 하나
  - 긴 시간 소요

## 목적함수 : 교차 엔트로피와 로그우도

### 평균제곱 오차 다시 생각하기

평균제곱 오차 목적함수

<img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191106125509485.png" alt="image-20191106125509485" style="zoom:67%;" />

- 오차가 클수록 $e$값이 크므로 벌점으로 훌륭함

- 큰 **허점**이 존재

  <img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191106125534193.png" alt="image-20191106125534193" style="zoom: 67%;" />

  - 왼쪽 상황은 $e = 0.2815$
  - 오른쪽 상황은 $e = 0.4971$, 오른쪽이 더 큰 벌점을 받아야 마땅함
    - 큰 교정이 필요함에도, **더 작은 그레이디언트가 적용됨**	
    - 더 낮은 벌점을 받은 꼴 -> 학습이 더디다

- **이유**

  ![image-20191106125831712](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191106125831712.png)

  - $wx+b$ (위 그래프의 가로축)가 커지면 그레디언트가 작아짐

### 교차 엔트로피 목적함수

- **교차 엔트로피**

  - 레이블에 해당하는 $y$가 확률변수 (부류가 2개라고 가정하면 $y \in \{0,1\}$

  - 확률 분포 : $P$는 정답 레이블, $Q$는 신경망 출력

    <img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111121123018.png" alt="image-20191111121123018" style="zoom: 67%;" />

  - 확률분포를 통일된 수식으로 쓰면

    ![image-20191111121225267](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111121225267.png)

  - 교차 엔트로피 $H(P,Q) = -\sum_xP(x)log_2Q(x)= -\sum_{i=1,k}P(e_i)log_2Q(e_i)$을 적용
    
    - $H(P,Q) = -\sum_{y\in \{0,1\}}P(y)log_2Q(y)$

- 손쉬운 예

  ![image-20191111121717707](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111121717707.png)

- 교차 엔트로피 목적함수

  $e = -(ylog_2o + (1-y)log_2(1-o)),$ 이때, $o = \sigma(z)이고 z = wx + b$ (5,4)

  - y가 1, o가 0.98일때(예측이 잘됨)
    - 오류 e = 0.0291로서 낮은 값
  - y가 1, o가 0.0001일때(예측이 잘못됨)
    - 오류 e = 13.2877로서 높은 값

- 공정한 벌점을 부여하는가

  ![image-20191111122407224](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111122407224.png)

  - 그레디언트를 계산해보면 오류가 더 큰 쪽에 더 큰 벌점 부과

    ![image-20191111122426533](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111122426533.png)

- 식 (5,4)를 **c개의 출력 노드를** 가진 경우로 **확장**

  - 출력 벡터 $\bold{o} = (o_1,o_2,...,o_c)^T$인 상황으로 확장

    ![image-20191111122538052](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111122538052.png)

### 소프트맥스 함수와 로그우도 목적함수

#### 소프트맥스 함수

![image-20191111122612962](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111122612962.png)

- 동작 예시

  - 소프트맥스는 최대를 모방

    - 출력 노드의 중간 계산 결과 $s_i^L$의 최대값을 더욱 활성화하고 다른 작은 값들은 억제
    - 모두 더하면 1이 되어 확률 모방

    ![image-20191111122720054](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111122720054.png)

    - 로지스틱 시그모이드로 바꾼 값은 확률분포일수가 없다
    - max가 1인 값만 1에 가깝도록 하고, 나머지는 0쪽으로 누르는 방식

#### 로그우도 목적함수

![image-20191111123318959](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111123318959.png)

- 모든 출력 노드값을 사용하는 MSE나 교차 엔트로피와는 달리 $o_y$라는 **하나의 노드만 적용**
  - 정답 노드만 보겠다는 뜻
- $o_y$는 샘플의 레이블에 해당하는 노드의 출력값
  - 그림 5-5에서 현재 샘플이 두번째 부류라면 $o_y = o_2$
    - 잘못 분류한 셈, 목적함수값이 **큼**
  - 그림 5-5에서 현재 샘플이 세번째 부류라면 $o_y = o_3$
    - 제대로 분류한 셈, 목적함수값이 **작음**

- 소프트맥스와 로그우도
  - 소프트맥스는 **최댓값이 아닌 값을 억제하여 0에 가깝게 만든다**는 의도 내포
  - 학습 샘플이 알려주는 부류에 **해당하는 노드만 보겠다는 로그우도**와 잘 어울림
  - 따라서 **둘을 결합하여 사용하는 경우가 많음**

#### 소프트맥스 분류기

- 다항 로지스틱 회귀분석의 예
- 분류기의 최종 값을 확률로 표현
- 소프트맥스와 로그우도 목적함수

![image-20191111124321905](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111124321905.png)

![image-20191111124505584](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111124505584.png)

#### 힌지로스

![image-20191111124740408](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111124740408.png)

- 확실한 분별력을 요할 때 주로 사용
- SVM : 분류할 때 마진을 크게 만듬(클래스 간의 간격이 벌어진다)

## 성능 향상을 위한 요령

### 데이터 전처리

- 규모 문제

  - 예시) 건강에 관련된 데이터 (키, 몸무게, 혈압)
    - 1.8과 1.5는 30cm나 차이나지만 특징값 차이는 불과 0.3
    - 60kg와 40kg는 20의 차이
    - 둘다 양수이며, 100배의 규모차이가 남
    - 키 특징에 연결된 가중치가 몸무게 특징보다 100배 느리게 학습됨(느린 학습의 요인)
  - 데이터들의 같은 차원에 있도록 크기를 맞춰줄 필요가 있다

- **<u>모든 특징이 양수인 경우의 문제</u>**(시험 나올듯)

  - $\delta_jz_i$가 그레디언트이므로

    ![image-20191111125923405](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111125923405.png)

    - :arrow_up: 표시된 가중치는 모두 증가, :arrow_down: 표시된 가중치는 모두 감소
    - 가중치가 뭉치로 증가 또는 감소하면 최저점을 찾아가는 경로가 갈팡질팡하여 수렴이 느림

- **정규화** : 규모문제와 양수문제 해결

  - 특징별로 독립적으로 적용

  - 통계학의 표준화 변환(z-transform)을 적용한 경우

    ![image-20191111131135462](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111131135462.png)

  - 일부의 경우, 최대 최소 변환을 적용한 경우

    ![image-20191111131202727](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111131202727.png)
  
- 전처리 과정 예

  ![image-20191113121559702](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191113121559702.png)

- **명목 변수**를 **원핫**코드로 변환

  - 명목 변수, 객체간 서로 구분하기 위한 변수

  - 거리개념이 없다

  - 값의 개수만큼 bit를 부여

  - 데이터 전처리의 핵심

    ![image-20191113123212919](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191113123212919.png)

  - 성별은 2비트, 체질은 4비트

  ![image-20191113123241033](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191113123241033.png)

### 가중치 초기화

- **대칭적 가중치** 문제

  <img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191113123308932.png" alt="image-20191113123308932" style="zoom:67%;" />

  - 위 그림의 대칭적 가중치에서는 $z_1^{l-1}$과 $z_2^{l-1}$과 같은 값이 됨
    - $-\delta_jz_i$가 그레이디언트이기 때문에 $u_{11}^l$과 $u_{12}^l$이 같은 값으로 갱신됨
    - 두 노드가 같은 일을 하는 중복성 발생
  - 난수 초기화를 통해 대칭 파괴

- **난수**로 가중치 초기화

  - 가우시안 분포 또는 균일 분포에서 난수 추출. 두 분포는 성능 차이 거의 없음

  - 난수 범위는 무척 중요함

    - 식 (5.10) 또는 식(5.11)로 r를 결정한 후 [-r, r] 사이에서 난수 발생

    `![image-20191113123818259](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191113123818259.png)

  - 바이어스는 보통 0으로 초기화

- 사례

  - AlexNet : 평균 0, 표준편차 0.01인 가우시안에서 난수 생성
  - ResNet : 평균 0, 표준편차 $\sqrt{\frac{2}{n_{in}}}$인 가우시안에서 난수 생성, 바이어스 0 설정

- 가중치 초기화에 따른 변화 예

  ![image-20191113123942628](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191113123942628.png)

### 모멘텀

- **그레이디언트의 잡음** 현상

  - 기계 학습은 훈련집합을 이용하여 그레이디언트를 추정하므로 잡음 가능성 높음
  - **모멘텀**은 그레이디언트에 스무딩을 가하여 잡음 효과 줄임
    - 관성 : 과거에 이동했던 방식을 기억하면서 기존 방향으로 일정 이상 추가 이동함
    - 수렴 속도 향상(지역 최저에 빠지는 문제 해소)

- **모멘텀을 적용한 가중치 갱신 수식**

  <img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191113125038721.png" alt="image-20191113125038721" style="zoom: 67%;" />

  - 속도 벡터 $v$는 이전 그레이디언트를 누적한 것에 해당함 (처음에는 $v = 0$으로 출발)

  - $\alpha$의 효과(관성의 정다)

    - $\alpha  = 0$이면 모멘텀이 적용 안 된 이전 공식과 같음

    - $\alpha$가 1에 가까울수록 이전 그레이디언트 정보에 큰 가중치를 주는 셈

      :arrow_forward: $\theta$가 그리는 궤적이 매끄러움

    - 보통 0.5, 0.9, 또는 0.99 사용

      (또는 0.5로 시작하여 세대가 지남에 따라 점점 키워 0.99에 도달하는 방법)

- **모멘텀의 효과**

  - 지나침 현상 누그러뜨림

    <img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191113125938470.png" alt="image-20191113125938470" style="zoom:50%;" />

- **네스테로프 가속 그레이디언트** 모멘텀

  - 현재 $v$값으로 다음 이동할 곳 $\hat{\theta}$을 예견한 후, 예견한 곳의 그레이디언트 ![image-20191113130055213](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191113130055213.png) 를 사용

    ![image-20191113130147492](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191113130147492.png)

    ![image-20191113130213280](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191113130213280.png)

  - 일반적으로 성능이 조금 더 좋다 

- 알고리즘

  - 일반적인 경사 하강 알고리즘

    <img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191113130450835.png" alt="image-20191113130450835" style="zoom:67%;" />

  - 네스테로프 모멘텀을 적용한 경사 하강 알고리즘

    <img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191113130509409.png" alt="image-20191113130509409" style="zoom:67%;" />

### 적응적 학습률

- **학습률** $p$의 중요성
  
- 너무 크면 지나침에 따른 진자 현상, 너무 작으면 수렴이 느림
  
![image-20191113130602354](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191113130602354.png)
  
- **적응적 학습률**

  - 그레이디언트에 학습률 p를 곱하면, ![image-20191113130654155](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191113130654155.png)

    즉, 모든 매개변수가 같은 크기의 학습률을 사용하는 셈

  - **적응적 학습률**은 **매개변수마다** 자신의 상황에 따라 **학습률을 조절**해 사용
    
    - 매개변수마다 학습률이 다르다
    - ex) 학습률 담금질
      이전 그레이디언트와 현재 그레이디언트의 부호가 같은 매개변수는 값을 키우고 다른 매개변수는 값을 줄이는 전략

- **AdaGrad** (Adaptive Gradient)

  - **시간에 따라서 줄어든다(누적)**
- **각각의 차원축에 따로 적용**
  
<img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191113131018373.png" alt="image-20191113131018373" style="zoom: 67%;" />
  
- 라인 5~7을 자세히 쓰면
  
  ![image-20191113131058835](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191113131058835.png)
  
    - 6번 라인 : normalize, 보폭을 정해주는 적응적 학습률
  
  - $r$은 이전 그레이디언트를 누적한 벡터
  
    - $r_i$가 크면 갱신값 $|\triangle \theta_i|$는 작아서 조금만 이동
    - $r_i$가 작으면 $|\triangle \theta_i|$는 커서 많이 이동
  
- **RMSProp**

  - Adagrad의 단점

    - [알고리즘 5-3]의 라인 5는 단순히 제곱을 더함

    - 따라서, 오래된 그레이디언트와 최근 그레이디언트는 **같은 비중**의 역할

      :arrow_forward: **$r$이 점점 커져 수렴 방해**할 가능성이 있다

  - **가중 이동 평균** 기법 적용

    $r = \alpha r + (1-\alpha )g\odot g$

    - $\alpha$가 작을수록 **최근 것에 비중**을 둠
    - 보통 $\alpha$로 0.9, 0.99, 0.999를 사용

  <img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191118120304147.png" alt="image-20191118120304147" style="zoom: 67%;" />

  - **과거의 데이터에 힘을 빼겠다는 의미가 강함**

- **Adam**(Adaptive moment)

  - RMSProp에 식 (5-12)의 **모멘텀을 추가**로 적용한 알고리즘

  <img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191118120407814.png" alt="image-20191118120407814" style="zoom:67%;" />

  - 일반적으로 $\alpha_1 = 0.9, \alpha_2 = 0.999, p = 1e-3$ 혹은 $5e-4$ 설정
  - 모멘텀, 적응적 학습률 모두 사용

- 동작 예시

  <img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191118120531264.png" alt="image-20191118120531264" style="zoom:67%;" />

  - [그림 5-13(a)]는 중앙으로 급강하하는 절벽 지형
  - [그림 5-13(b)]는 중앙 부근에 안장점이 있는 지형
  - SGD, SGD+Momentum, Adagrad, RMSProp, Adam 모두 학습률을 하이퍼매개변수로 가짐
  - NAG (네스테로프)
  - 최적의 러닝레이트를 직접 찾는것이 가장 좋은 결과를 낳지만, 현실적으로 힘들어 사용

### 활성함수

- 선형 연산 결과인 활성값 $z$를 계산하고 비선형 활성함수 $\tau$를 적용하느 ㄴ과정

  ![image-20191118125534889](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191118125534889.png)

  - tanh는 활성값이 커지면 포화상태가 되고, 그레이디언트는 0에 가까워짐
    - 매개변수 갱신(학습)이 매우 느린 요인

- **ReLU**

  - 그레이디언트 포화문제 해소
  - 문제점 : 평균점이 0보다 크다
    - **양의 문제** 발생 가능성이 있음
    - ReLU에 0이 한번 걸리면 신호가 전달되지 않을 것이다
    - 초기화를 잘못해서 ReLU에 0으로 한번 걸리면 계속 0이 돼버린다

- ReLU 변형

  - Leaky ReLU(보통 $\alpha$ = 0.01을 사용)

    ![image-20191118125804960](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191118125804960.png)

  - PReLU($\alpha$를 학습으로 알아냄)

    <img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191118125719658.png" alt="image-20191118125719658" style="zoom:67%;" />

- 다양한 활성함수

  ![image-20191118130801410](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191118130801410.png)

### 배치 정규화

- **공변량 시프트** 현상

  - 학습이 진행되면서 층1의 매개변수가 바뀜에 따라 ![image-20191118130837056](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191118130837056.png)이 따라 바뀜

    - 층2 입장에서 보면 자신에게 입력되는 **데이터의 분포가 수시로 바뀌는 셈**

  - 층2, 층3, ... 으로 깊어짐에 따라 더욱 심각

  - **학습을 방해하는 요인**으로 작용

    <img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191118130932080.png" alt="image-20191118130932080" style="zoom:67%;" />

- **배치 정규화**

  - 공변량 시프트 현상을 누그러뜨리기 위해 식 (5.9)의 **정규화를 모든 층에 적용**하는 기법

    <img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191118131211734.png" alt="image-20191118131211734" style="zoom:67%;" />

  - 정규화를 적용하는 곳이 중요

    - 식 (5.15)의 연산 과정 중 식 (5.9)를 어디에 적용하나? (**적용 위치**)

      <img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191118131244270.png" alt="image-20191118131244270" style="zoom:67%;" /><img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191118131250432.png" alt="image-20191118131250432" style="zoom:67%;" />

    - 입력 ![image-20191118131324108](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191118131324108.png) 또는 중간 결과 $z$ 중 어느 것에 적용? :arrow_forward: **$z$에 적용하는 것이 유리**

    - 일반적으로 완전 연결층, 컨볼루셔층 직후 혹은 **비선형 함수 적용 직전** 적용

  - 훈련집합 전체 또는 미니배치 중 어느 것에 적용? (**적용 단위**)

    - **미니배치에 적용하는 것이 유리**

