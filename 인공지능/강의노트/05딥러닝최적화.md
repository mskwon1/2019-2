# 딥러닝 최적화

- 과학과 공학에서 최적화
- 기계학습의 최적화도 매우 복잡함
  - 훈련집합으로 학습을 마친 후, 현장에서 발생하는 새로운 샘플을 잘 예측해야 함
    - 즉 **일반화 능력**이 좋아야 함
    - **훈련집합**은 전체 데이터(실제로 알 수 없음)의 대리자 역할
    - **검증집합**은 테스트집합의 대리자 역할
    - 오차제곱의 평균, 로그우도와 같은 **목적함수**도 궁극 목표인 정확률(판단 기준)의 대리자 역할
- 기계학습의 최적화가 어려운 이유
  - 대리자 관계
  - 목적함수의 비볼록 성질, 고차원 특징 공간, 데이터의 희소성 등
    - 볼록 성질을 가지는 목적함수도 있긴하다(MSE) - 최소값이 하나
  - 긴 시간 소요

## 목적함수 : 교차 엔트로피와 로그우도

### 평균제곱 오차 다시 생각하기

평균제곱 오차 목적함수

<img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191106125509485.png" alt="image-20191106125509485" style="zoom:67%;" />

- 오차가 클수록 $e$값이 크므로 벌점으로 훌륭함

- 큰 **허점**이 존재

  <img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191106125534193.png" alt="image-20191106125534193" style="zoom: 67%;" />

  - 왼쪽 상황은 $e = 0.2815$
  - 오른쪽 상황은 $e = 0.4971$, 오른쪽이 더 큰 벌점을 받아야 마땅함
    - 큰 교정이 필요함에도, **더 작은 그레이디언트가 적용됨**	
    - 더 낮은 벌점을 받은 꼴 -> 학습이 더디다

- **이유**

  ![image-20191106125831712](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191106125831712.png)

  - $wx+b$ (위 그래프의 가로축)가 커지면 그레디언트가 작아짐

### 교차 엔트로피 목적함수

### 소프트맥스 함수와 로그우도 목적함수