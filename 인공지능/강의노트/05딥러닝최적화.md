# 딥러닝 최적화

- 과학과 공학에서 최적화
- 기계학습의 최적화도 매우 복잡함
  - 훈련집합으로 학습을 마친 후, 현장에서 발생하는 새로운 샘플을 잘 예측해야 함
    - 즉 **일반화 능력**이 좋아야 함
    - **훈련집합**은 전체 데이터(실제로 알 수 없음)의 대리자 역할
    - **검증집합**은 테스트집합의 대리자 역할
    - 오차제곱의 평균, 로그우도와 같은 **목적함수**도 궁극 목표인 정확률(판단 기준)의 대리자 역할
- 기계학습의 최적화가 어려운 이유
  - 대리자 관계
  - 목적함수의 비볼록 성질, 고차원 특징 공간, 데이터의 희소성 등
    - 볼록 성질을 가지는 목적함수도 있긴하다(MSE) - 최소값이 하나
  - 긴 시간 소요

## 목적함수 : 교차 엔트로피와 로그우도

### 평균제곱 오차 다시 생각하기

평균제곱 오차 목적함수

<img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191106125509485.png" alt="image-20191106125509485" style="zoom:67%;" />

- 오차가 클수록 $e$값이 크므로 벌점으로 훌륭함

- 큰 **허점**이 존재

  <img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191106125534193.png" alt="image-20191106125534193" style="zoom: 67%;" />

  - 왼쪽 상황은 $e = 0.2815$
  - 오른쪽 상황은 $e = 0.4971$, 오른쪽이 더 큰 벌점을 받아야 마땅함
    - 큰 교정이 필요함에도, **더 작은 그레이디언트가 적용됨**	
    - 더 낮은 벌점을 받은 꼴 -> 학습이 더디다

- **이유**

  ![image-20191106125831712](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191106125831712.png)

  - $wx+b$ (위 그래프의 가로축)가 커지면 그레디언트가 작아짐

### 교차 엔트로피 목적함수

- **교차 엔트로피**

  - 레이블에 해당하는 $y$가 확률변수 (부류가 2개라고 가정하면 $y \in \{0,1\}$

  - 확률 분포 : $P$는 정답 레이블, $Q$는 신경망 출력

    <img src="C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111121123018.png" alt="image-20191111121123018" style="zoom: 67%;" />

  - 확률분포를 통일된 수식으로 쓰면

    ![image-20191111121225267](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111121225267.png)

  - 교차 엔트로피 $H(P,Q) = -\sum_xP(x)log_2Q(x)= -\sum_{i=1,k}P(e_i)log_2Q(e_i)$을 적용
    - $H(P,Q) = -\sum_{y\in \{0,1\}}P(y)log_2Q(y)$

- 손쉬운 예

  ![image-20191111121717707](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111121717707.png)

- 교차 엔트로피 목적함수

  $e = -(ylog_2o + (1-y)log_2(1-o)),$ 이때, $o = \sigma(z)이고 z = wx + b$ (5,4)

  - y가 1, o가 0.98일때(예측이 잘됨)
    - 오류 e = 0.0291로서 낮은 값
  - y가 1, o가 0.0001일때(예측이 잘못됨)
    - 오류 e = 13.2877로서 높은 값

- 공정한 벌점을 부여하는가

  ![image-20191111122407224](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111122407224.png)

  - 그레디언트를 계산해보면 오류가 더 큰 쪽에 더 큰 벌점 부과

    ![image-20191111122426533](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111122426533.png)

- 식 (5,4)를 **c개의 출력 노드를** 가진 경우로 **확장**

  - 출력 벡터 $\bold{o} = (o_1,o_2,...,o_c)^T$인 상황으로 확장

    ![image-20191111122538052](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111122538052.png)

### 소프트맥스 함수와 로그우도 목적함수

#### 소프트맥스 함수

![image-20191111122612962](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111122612962.png)

- 동작 예시

  - 소프트맥스는 최대를 모방

    - 출력 노드의 중간 계산 결과 $s_i^L$의 최대값을 더욱 활성화하고 다른 작은 값들은 억제
    - 모두 더하면 1이 되어 확률 모방

    ![image-20191111122720054](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111122720054.png)

    - 로지스틱 시그모이드로 바꾼 값은 확률분포일수가 없다
    - max가 1인 값만 1에 가깝도록 하고, 나머지는 0쪽으로 누르는 방식

#### 로그우도 목적함수

![image-20191111123318959](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111123318959.png)

- 모든 출력 노드값을 사용하는 MSE나 교차 엔트로피와는 달리 $o_y$라는 **하나의 노드만 적용**
  - 정답 노드만 보겠다는 뜻
- $o_y$는 샘플의 레이블에 해당하는 노드의 출력값
  - 그림 5-5에서 현재 샘플이 두번째 부류라면 $o_y = o_2$
    - 잘못 분류한 셈, 목적함수값이 **큼**
  - 그림 5-5에서 현재 샘플이 세번째 부류라면 $o_y = o_3$
    - 제대로 분류한 셈, 목적함수값이 **작음**

- 소프트맥스와 로그우도
  - 소프트맥스는 **최댓값이 아닌 값을 억제하여 0에 가깝게 만든다**는 의도 내포
  - 학습 샘플이 알려주는 부류에 **해당하는 노드만 보겠다는 로그우도**와 잘 어울림
  - 따라서 **둘을 결합하여 사용하는 경우가 많음**

#### 소프트맥스 분류기

- 다항 로지스틱 회귀분석의 예
- 분류기의 최종 값을 확률로 표현
- 소프트맥스와 로그우도 목적함수

![image-20191111124321905](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111124321905.png)

![image-20191111124505584](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111124505584.png)

#### 힌치로스

![image-20191111124740408](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111124740408.png)

- SVM : 분류할 때 마진을 크게 만듬(클래스 간의 간격이 벌어진다)

## 성능 향상을 위한 요령

### 데이터 전처리

- 규모 문제

  - 예시) 건강에 관련된 데이터 (키, 몸무게, 혈압)
    - 1.8과 1.5는 30cm나 차이나지만 특징값 차이는 불과 0.3
    - 60kg와 40kg는 20의 차이
    - 둘다 양수이며, 100배의 규모차이가 남
    - 키 특징에 연결된 가중치가 몸무게 특징보다 100배 느리게 학습됨(느린 학습의 요인)

- 모든 특징이 양수인 경우의 문제

  - $\delta_jz_i$가 그레디언트이므로

    ![image-20191111125923405](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111125923405.png)

    - :arrow_up: 표시된 가중치는 모두 증가, :arrow_down: 표시된 가중치는 모두 감소
    - 가중치가 뭉치로 증가 또는 감소하면 최저점을 찾아가는 경로가 갈팡질팡하여 수렴이 느림

- **정규화** : 규모문제와 양수문제 해결

  - 특징별로 독립적으로 적용

  - 통계학의 표준화 변환(z-transform)을 적용한 경우

    ![image-20191111131135462](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111131135462.png)

  - 일부의 경우, 최대 최소 변환을 적용한 경우

    ![image-20191111131202727](C:\Users\user\AppData\Roaming\Typora\typora-user-images\image-20191111131202727.png)

### 가중치 초기화

### 모멘텀

### 적응적 학습률

### 활성함수

### 배치 정규화