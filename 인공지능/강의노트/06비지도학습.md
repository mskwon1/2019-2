# 비지도학습

## 지도학습 / 비지도학습 / 준지도학습

- 지도 학습 : 모든 훈련 샘플이 레이블 정보를 가짐
- 비지도 학습 : 모든 훈련 샘플이 레이블 정보를 가지지 않음
- 준지도 학습 : 레이블을 가진 샘플과 가지지 않은 샘플이 섞여 있음

<img src="../../typora_images/06비지도학습/image-20191127125839300.png" alt="image-20191127125839300" style="zoom:67%;" />

- 기계 학습이 사용하는 두 종류의 지식
  - 훈련집합을 통한 전체 데이터에 대한 지식
  - 사전 지식(세상의 일반적인 규칙)
- 중요한 두가지 **사전 지식**
  - 매니폴드 가정 : 데이터 집합은 하나의 매니폴드 또는 여러 개의 매니폴드를 구성하며, 모든 샘플은 매니폴드와 가까운 곳에 있다
  - 매끄러움 가정 : 샘플은 어떤 요인에 의해 변화한다. 예를 들어, 장면과 카메라 위치를 고정한 상태에서 조명을 조금씩 변화하면서 영상을 획득한 경우, 획득된 영상 샘플은 특징 공간에서 위치가 조금씩 바뀔 것이다. 매끄러운 곡면을 따라 위치가 변함
- 비지도 학습 / 준지도 학습은 **사전 지식을 더 명시적으로 사용**
  - 매니폴드와 매끄러움 사전 지식에 의해 우리가 학습하는 함수는 작은 영역 내에서는 많이 변하지 않음을 가정함 $f(x) \approx f(x+\epsilon), \epsilon$ : 작은 변화
  - 예로 좋은 입력들의 경우, 정답이 같으면 유사한 입력 특징 공간에 존재할 확률이 높음

## 비지도학습

### 비지도 학습의 일반 과업

- 세 가지 일반 **과업**

  - **군집화** : 유사한 샘플을 모아 같은 그룹으로 묶는 일
  - **밀도 추정** : 데이터로부터 **확률분포를 추정**하는 일
  - **공간 변환** : 원래 특징공간을 **저차원 또는 고차원 공간으로 변환**하는 일
    - 원래 가지고 있던 공간에 대한 이해를 토대로 변환

- 데이터에 내재한 구조를 잘 파악하여 새로운 정보를 발견해야 함

  <img src="../../typora_images/06비지도학습/image-20191127130647096.png" alt="image-20191127130647096" style="zoom:67%;" />

### 비지도 학습의 응용 과업

- 아주 많은 응용
  - **군집화**의 응용
    - 영상 분할
    - 유전자 데이터 분석
    - 맞춤 광고, SNS 실시간 검색어 분석을 통해 사람들의 관심 파악하여 제안
  - **밀도 추정**의 응용
    - 분류, 생성 모델 구축 등
  - **공간 변환**의 응용
    - 데이터 가시화, 데이터 압축, 특징 추출(표현 학습) 등

## 군집화

### k-평균 알고리즘

- 군집화 **문제**

  $X = \{x_1,x_2,...,x_n\}$에서 식 (6.1)을 만족하는 **군집** 집합 $C = \{c_1,c_2,...,c_k\}$를 **찾아내는 작업**

  - 군집의 개수 $k$는 주어지는 경우와 자동으로 찾아야 하는 경우가 있음
  - 군집화를 부류 발견 작업이라 부르기도 함

- 군집화의 **주관성** 

  - 군집의 개수, 최적의 거리 측정 방식을 설정해야 함

  <img src="../../typora_images/06비지도학습/image-20191202121235527.png" alt="image-20191202121235527" style="zoom:67%;" />

- 군집의 개수 $k$

  - 주관에 의해 결정

- **거리 측정법**

  ![image-20191202121341669](../../typora_images/06비지도학습/image-20191202121341669.png)

  - L1 (Manhattan) Distance
  - L2 (Euclidean) Distance
  - 어떤 것을 선택하느냐에 따라 바운더리가 달라진다
    - 정해진 정답이 없음

- **분류**

  <img src="../../typora_images/06비지도학습/image-20191202121513412.png" alt="image-20191202121513412" style="zoom:67%;" />

  - 연결 기준 군집화 (ex) single-linkage clustering
    - 인접한 거리 기준으로 묶어가는 가는 것
    - $k$개로 나누고 싶다 :arrow_forward: hierarchical clustering dendrogram의 상위 $k$개로 분류
  - 중심 기준 군집화 (ex) k-means clustering
    - 가장 기본적인 군집화 방법
    - 중심점을 찾고, 그걸 기준으로 묶어가는 것
  - 분포 기준 군집화 (ex) gaussian mixture model clustering
    - 확률 분포 활용
  - 밀도 기준 군집화 (ex) DBSCAN
    - 어느 범주 안에서 밀도 기준으로 군집화

- $k$-**평균 군집화** 알고리즘의 특성

  - 원리 단순하지만 성능이 좋아 인기가 좋음
  - 직관적으로 이해하기 쉽고 구현 쉬움
  - 군집 개수 $k$, 거리측정 방법을 설정해야 함

  <img src="../../typora_images/06비지도학습/image-20191202122157661.png" alt="image-20191202122157661" style="zoom:67%;" />

  ![image-20191202122703497](../../typora_images/06비지도학습/image-20191202122703497.png)

  - 임의의 k개 점을 잡고(군집 중심), 할당된 점에 가까운 점들을 할당시켜줌
  - 각 군집별로 평균값을 구하고, 중심점을 옮김
    - 반복해서 새롭게 계산(어느정도 수렴이 될 때까지)

- $k$-평균과 $k$-**중앙객체** 군집

  - $k$-평균은 [알고리즘 6-1]의 라인 7에서 **샘플의 평균으로 군집 중심을 갱신** (잡음에 민감)

  - $k$-중앙객체는 **실제 존재하는 객체들 중 하나를 뽑아 대표 객체로 선정**하고 이 객체를 중심으로 군집 중심을 갱신 ($k$-평균에 비해 **잡음에 둔감**)

  - 중앙객체 : 객체 집합에서 수학적으로 대표적인 객체

    <img src="../../typora_images/06비지도학습/image-20191202123046708.png" alt="image-20191202123046708" style="zoom:67%;" />

    - 평균에 가장 가까운 요소(Outlier 때문에 평균값이 끌려가는걸 방지)

- 최적화 문제로 해석

  - $k$-평균은 식 (6.2)의 목적함수를 최소화하는 알고리즘

  - 행렬 $\bold{A}$는 군집 배정 정보를 나타내는 $k*n$행렬

    ($i$번째 샘플이 $j$번째 군집에 배정되었다면 $a_{ji}$는 1, 그렇지 않으면 0)

    <img src="../../typora_images/06비지도학습/image-20191202123220325.png" alt="image-20191202123220325" style="zoom:67%;" />

![image-20191202122827375](../../typora_images/06비지도학습/image-20191202122827375.png)

![image-20191202122835911](../../typora_images/06비지도학습/image-20191202122835911.png)

- 다중 시작 $k$-평균

  - $k$-평균은 [알고리즘 6-1]의 라인 1에서 **초기 군집 중심이 달라지면 최종 결과가 달라짐**
  - 다중 시작은 서로 다른 초기 군집 중심을 가지고 **여러 번 수행**한 다음, 
    **가장 좋은 품질의 해를 취함**

  <img src="../../typora_images/06비지도학습/image-20191202123359544.png" alt="image-20191202123359544" style="zoom:67%;" />

- EM(Expectation Maximization) 기초

  - $k$-평균에서 훈련집합 $X$와 군집집합 $C$(행렬 $\bold{A}$)는 각각 입력단과 출력단에서 관찰 가능

  - 중간 단계의 임시 변수 $Z$ (입출력단에서 보이지 않기 때문에 은닉변수라 부름)

  - $k$**-평균의 할당과 갱신과정은**

    $Z$의 추정 (E 단계)과 $\bold{A}$의 추정 (M 단계)을 번갈아 가면 수행하는 **EM 알고리즘과 유사**

    <img src="../../typora_images/06비지도학습/image-20191202123600199.png" alt="image-20191202123600199" style="zoom: 67%;" />

### 친밀도 전파 알고리즘

- **친밀도 전파** 알고리즘

  - **데이터들간의 신호 전달 개념을 사용**
    - 모든 데이터가 **친밀도 기준에 따라 자신을 대표할 중심 데이터를 선택하고 군집화**함
  - **책임 행렬 $\bold{R}$과 가용 행렬 $\bold{A}$라는 두 종류의 친밀도 행렬**을 이용하여 군집화
    - $r_{ik}$ : 데이터 $x_k$가 데이터 $x_i$의 **대표가 되어야 한다는 책임의 근거**
    - $a_{ik}$ : 데이터 $x_i$가 데이터 $x_k$를 **대표로 선택될 가능성의 근거**
  - **군집 개수 $k$를 자동으로 추정**하여 알아냄 ($k$-평균 군집의 단점 해소)

- 샘플 $i$와 $k$의 유사도 $s_{ik}$

  <img src="../../typora_images/06비지도학습/image-20191202124512726.png" alt="image-20191202124512726" style="zoom:67%;" />

  - **유사도를 이용하여 책임 행렬과 가용 행렬을 계산**
  - 유클리디언 거리의 제곱에 음수
    - 가까울수록, 즉 유사할수록 큰 값을 가짐

- 책임 행렬 $\bold{R}$과 가용 행렬 $\bold{A}$의 계산

  <img src="../../typora_images/06비지도학습/image-20191202124559472.png" alt="image-20191202124559472" style="zoom:67%;" />

  <img src="../../typora_images/06비지도학습/image-20191202124604945.png" alt="image-20191202124604945" style="zoom:67%;" />

  <img src="../../typora_images/06비지도학습/image-20191202124609655.png" alt="image-20191202124609655" style="zoom:67%;" />

- 자가 유사도 $s_{kk}$

  - 유사도의 최솟값, 중앙값, 최댓값 중에서 선택 (하이퍼 매개변수임)
  - 최솟값은 적은 수의 군집, 최댓값은 많은 수의 군집을 생성, 중앙값은 중간 정도

- 자가 친밀도 $r_{kk}$과 $a_{kk}$

  - $r_{kk}$는 식 (6.4)를 그대로 사용. 즉 $r_{kk} = s_{kk} - max_{k' \ne k}(a_{kk'}+s_{kk'})$

  - $a_{kk}$는 (6.6)으로 계산

    <img src="../../typora_images/06비지도학습/image-20191202124805768.png" alt="image-20191202124805768" style="zoom:67%;" />

![image-20191202124821998](../../typora_images/06비지도학습/image-20191202124821998.png)

- 알고리즘 세부 과정

  ![image-20191202124832938](../../typora_images/06비지도학습/image-20191202124832938.png)

  ![image-20191202124836045](../../typora_images/06비지도학습/image-20191202124836045.png)

  - (B) : **"책임"** $r(i,k)$ : 데이터 포인트 :arrow_forward: 후보 표본
    - 각 데이터 포인트가 다른 후보 표본보다 **후보 표본을 선호하는 정도**

  - (C) : **"가용도"** $a(i,k)$ : 후보 표본 :arrow_forward: 데이터 포인트
    - 각 후보 표본이 데이터 포인트의 **군집 중심으로 사용될 수 있는 정도**

## 밀도 추정

- 밀도 추정 문제

  - **모수적 추정** : 정해진 확률 밀도 함수로 추정 (몇 개의 매개변수로 확률 분포 정의)

  - **비모수적 추정** : 관측치만으로 데이터의 분포를 추정

  - 어떤 점 $x$에서 데이터가 발생할 확률, 즉 **확률분포 $P(x)$를 구하는 문제**

    <img src="../../typora_images/06비지도학습/image-20191202125133859.png" alt="image-20191202125133859" style="zoom: 67%;" />

    - $P(x_1) \gt P(x_2) \gt P(x_3)$

### 커널 밀도 추정

- **히스토그램 추정법**

  - 특징 공간을 칸의 집합으로 분한할 다음, 칸에 있는 샘플의 빈도를 세어 식(6.7)로 추정

    <img src="../../typora_images/06비지도학습/image-20191202125240926.png" alt="image-20191202125240926" style="zoom:67%;" />

  - 여러 문제점

    - **매끄럽지 못하고 (경계에서의 불연속성) 계단 모양**을 띠는 확률밀도함수가 됨
    - 칸의 크기와 위치에 **민감함**

- **커널 밀도 추정법** (비모수 방법)

  - 커널함수를 이용한 확률 밀도 함수를 만들어 **히스토그램 추정의 단점을 개선**
    - 커널(알맹이) : 원점 대칭이면서 1인 적분값을 가지는 함수로 정의
  - 점 $\bold{x}$에 [그림 6-9]가 예시하는 커널을 씌우고 커널 안에 있는 샘플의 가중합을 이용함
  - 대역폭 $h$의 크기가 중요

  ![image-20191202125551396](../../typora_images/06비지도학습/image-20191202125551396.png)

- 히스토그램 방법과 커널 밀도 추정법의 비교

  - 관측된 샘플마다 해당 값을 중심으로 하는 커널 함수 생성 $K_h(x-x_i)$

  - 얻어진 커널 함수의 결과를 더하여 전체 개수로 난눔

    - 커널 밀도 추정법은 매끄러운 확률밀도함수를 추정함

    <img src="../../typora_images/06비지도학습/image-20191202125648052.png" alt="image-20191202125648052" style="zoom:67%;" />

- 커널 밀도 추정법에서 **대역폭 $h$의 중요성**

  - $h$가 너무 작으면 (빨강) 뾰족뾰족한 모양, $h$가 너무 크면 (녹색) 뭉개짐, 적절하게 설정 (검정)

    <img src="../../typora_images/06비지도학습/image-20191202125757629.png" alt="image-20191202125757629" style="zoom: 50%;" />

- 커널 밀도 추정 기법의 근본적 문제점

  - 샘플을 모두 저장하고 있어야 하는 메모리 기반 방법
    (새로운 샘플이 주어질 때마다 식 (6.8)을 처음부터 다시 계산)
  - 데이터 희소성(차원의 저주)
    - 데이터가 낮은 차원인 경우로 국한하여 활용

- **커널 함수**

  - **커널 밀도 추정법**의 예와 같이 확률 밀도 추정에서 활용

  - **차원 변환**에서도 활용

    - 원공간에서 선형 분리가 안되는 데이터들을 공간 변형을 통해 고차원 공간으로 옮기고 선형 분리되도록 함

    <img src="../../typora_images/06비지도학습/image-20191202130134718.png" alt="image-20191202130134718" style="zoom:50%;" />

    커널 대치

    - 서포트 벡터머신에서 활용됨
    - 실제 데이터를 새로운 공간으로 변형하지 않고 변형된 공간의 데이터간의 거리를 구함
      (ex) 다항식 커널, 가우시안 커널

### 가우시안 혼합

- **가우시안을 이용한 방법** (모수적 방법)

  - 데이터가 **가우시안 분포를 따른다고 가정하고** 평균 벡터 $\mu$와 **공분산** 행렬 $\sum$을 **추정함**

    <img src="../../typora_images/06비지도학습/image-20191202130421070.png" alt="image-20191202130421070" style="zoom:67%;" />

- 대부분 데이터가 하나의 가우시안으로 불충분(오른쪽)

  <img src="../../typora_images/06비지도학습/image-20191202130442222.png" alt="image-20191202130442222" style="zoom:50%;" />

- **가우시안 혼합**

  - [그림 6-13]은 2개의 가우시안을 사용한 예

    ![image-20191202130532131](../../typora_images/06비지도학습/image-20191202130532131.png)

    - 가우시안 간의 가중치를 정해주게 됨(퍼센티지)
    - 가우시안 별로 (평균, 분산, 가중치)만 알면 됨

  - $k$개의 가우시안으로 일반화하면,

    - 확률 분포 $P(x)$는 $k$개 가우시안의 선형 결합으로 표현

      <img src="../../typora_images/06비지도학습/image-20191202130557409.png" alt="image-20191202130557409" style="zoom:67%;" />

- 주어진 데이터와 추정해야 할 매개변수를 정리하면,

  <img src="../../typora_images/06비지도학습/image-20191202130614699.png" alt="image-20191202130614699" style="zoom:67%;" />

- **최대 우도**를 이용한 최적화 문제로 공식화

  <img src="../../typora_images/06비지도학습/image-20191202130720204.png" alt="image-20191202130720204" style="zoom:50%;" />

  <img src="../../typora_images/06비지도학습/image-20191202130735902.png" alt="image-20191202130735902" style="zoom:50%;" />

  - **주어진 X가 발생할 가능성이 가장 큰 $\theta$를 찾는 문제**

### EM 알고리즘

- **EM 알고리즘을 이용**한 식 (6.13)의 풀이

  - $\theta$를 모르므로 난수로 설정하고 출발

    ![image-20191202130844072](../../typora_images/06비지도학습/image-20191202130844072.png)

  - 과정 : 가우시안으로 샘플의 소속 정보 개선 (E단계)

    - 샘플의 소속 정보로 가우시안 개선 (M단계)
    - 가우시안으로 샘플의 소속 정보 개선 (E단계)
    - 샘플의 소속 정보로 가우시안 개선 (M단계)
    - ...

- 가우시안 혼합을 위한 EM 알고리즘

  <img src="../../typora_images/06비지도학습/image-20191202131519295.png" alt="image-20191202131519295" style="zoom:67%;" />

  - 라인 3과 라인 4를 위한 수식

  - $z_{ji}$는 $\bold{x_i}$가 $j$번째 가우시안에 속할 확률

    ![image-20191202131608224](../../typora_images/06비지도학습/image-20191202131608224.png)

  